{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Embedding,Dense,Conv1D,Dropout,MaxPooling1D,Flatten\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = fetch_20newsgroups(subset='train').data\n",
    "train_label = fetch_20newsgroups(subset='train').target\n",
    "test_text = fetch_20newsgroups(subset='test').data\n",
    "test_label = fetch_20newsgroups(subset='test').target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Constatns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words_to_keep = 5000\n",
    "maxlen_text = 200\n",
    "token_vec_size = 128\n",
    "output_dim = np.unique(train_label).__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pad each newsgroups posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words_to_keep,filters='!\"#$%&()*+,\\'-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\\"',\n",
    "                        lower=True,\n",
    "                        split=\" \",\n",
    "                        char_level=False)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_text)\n",
    "train_X = pad_sequences(sequences=sequences,maxlen=maxlen_text)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(test_text)\n",
    "test_X = pad_sequences(sequences=sequences,maxlen=maxlen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle the Label using LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(train_label)\n",
    "\n",
    "encoded_train_Y = encoder.transform(train_label)\n",
    "train_Y = np_utils.to_categorical(encoded_train_Y,num_classes=output_dim)\n",
    "\n",
    "encoded_train_Y = encoder.transform(test_label)\n",
    "test_Y = np_utils.to_categorical(encoded_train_Y,num_classes=output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets print the shapes of data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 200)\n",
      "(11314, 20)\n",
      "(7532, 200)\n",
      "(7532, 20)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(input_dim=max_words_to_keep, output_dim=token_vec_size, input_length=maxlen_text))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "cnn_model.add(Conv1D(64, 5, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(pool_size=4))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(units=output_dim,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7919 samples, validate on 3395 samples\n",
      "Epoch 1/20\n",
      "7919/7919 [==============================] - 5s - loss: 2.9804 - acc: 0.0638 - val_loss: 2.9574 - val_acc: 0.0834\n",
      "Epoch 2/20\n",
      "7919/7919 [==============================] - 0s - loss: 2.9139 - acc: 0.1217 - val_loss: 2.9242 - val_acc: 0.1037\n",
      "Epoch 3/20\n",
      "7919/7919 [==============================] - 0s - loss: 2.8723 - acc: 0.1505 - val_loss: 2.8941 - val_acc: 0.1090\n",
      "Epoch 4/20\n",
      "7919/7919 [==============================] - 0s - loss: 2.8085 - acc: 0.1818 - val_loss: 2.8370 - val_acc: 0.1517\n",
      "Epoch 5/20\n",
      "7919/7919 [==============================] - 0s - loss: 2.7068 - acc: 0.2551 - val_loss: 2.7245 - val_acc: 0.1770\n",
      "Epoch 6/20\n",
      "7919/7919 [==============================] - 0s - loss: 2.5352 - acc: 0.3359 - val_loss: 2.5394 - val_acc: 0.2359\n",
      "Epoch 7/20\n",
      "7919/7919 [==============================] - 0s - loss: 2.3014 - acc: 0.4019 - val_loss: 2.3255 - val_acc: 0.2931\n",
      "Epoch 8/20\n",
      "7919/7919 [==============================] - 0s - loss: 2.0337 - acc: 0.4667 - val_loss: 2.1098 - val_acc: 0.3314\n",
      "Epoch 9/20\n",
      "7919/7919 [==============================] - 0s - loss: 1.7585 - acc: 0.5444 - val_loss: 1.9010 - val_acc: 0.3950\n",
      "Epoch 10/20\n",
      "7919/7919 [==============================] - 0s - loss: 1.5024 - acc: 0.6119 - val_loss: 1.7109 - val_acc: 0.4430\n",
      "Epoch 11/20\n",
      "7919/7919 [==============================] - 0s - loss: 1.2695 - acc: 0.6881 - val_loss: 1.5591 - val_acc: 0.5001\n",
      "Epoch 12/20\n",
      "7919/7919 [==============================] - 0s - loss: 1.0559 - acc: 0.7562 - val_loss: 1.4274 - val_acc: 0.5293\n",
      "Epoch 13/20\n",
      "7919/7919 [==============================] - 0s - loss: 0.8750 - acc: 0.8033 - val_loss: 1.3110 - val_acc: 0.5688\n",
      "Epoch 14/20\n",
      "7919/7919 [==============================] - 0s - loss: 0.7253 - acc: 0.8454 - val_loss: 1.2171 - val_acc: 0.6000\n",
      "Epoch 15/20\n",
      "7919/7919 [==============================] - 0s - loss: 0.6045 - acc: 0.8765 - val_loss: 1.1625 - val_acc: 0.6224\n",
      "Epoch 16/20\n",
      "7919/7919 [==============================] - 0s - loss: 0.5036 - acc: 0.8991 - val_loss: 1.1041 - val_acc: 0.6409\n",
      "Epoch 17/20\n",
      "7919/7919 [==============================] - 0s - loss: 0.4176 - acc: 0.9264 - val_loss: 1.0636 - val_acc: 0.6542\n",
      "Epoch 18/20\n",
      "7919/7919 [==============================] - 0s - loss: 0.3501 - acc: 0.9408 - val_loss: 1.0425 - val_acc: 0.6622\n",
      "Epoch 19/20\n",
      "7919/7919 [==============================] - 0s - loss: 0.2938 - acc: 0.9554 - val_loss: 1.0174 - val_acc: 0.6710\n",
      "Epoch 20/20\n",
      "7919/7919 [==============================] - 0s - loss: 0.2461 - acc: 0.9679 - val_loss: 1.0036 - val_acc: 0.6786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e15ba4518>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(train_X, train_Y, batch_size=1000,validation_split=0.3, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prediction on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6944/7532 [==========================>...] - ETA: 0s[12  1 19 ...,  7  4 15]\n"
     ]
    }
   ],
   "source": [
    "res_class_cnn = cnn_model.predict_classes(test_X)\n",
    "print(res_class_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24981001622\n",
      "0.605549654806\n"
     ]
    }
   ],
   "source": [
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(test_X,test_Y,verbose=2)\n",
    "print(cnn_loss)\n",
    "print(cnn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=max_words_to_keep, output_dim=token_vec_size, input_length=maxlen_text))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(units=token_vec_size, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model.add(Dense(units=output_dim, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7919 samples, validate on 3395 samples\n",
      "Epoch 1/20\n",
      "7919/7919 [==============================] - 5s - loss: 2.9922 - acc: 0.0787 - val_loss: 2.9870 - val_acc: 0.1046\n",
      "Epoch 2/20\n",
      "7919/7919 [==============================] - 4s - loss: 2.9741 - acc: 0.1443 - val_loss: 2.9618 - val_acc: 0.0890\n",
      "Epoch 3/20\n",
      "7919/7919 [==============================] - 5s - loss: 2.9225 - acc: 0.1874 - val_loss: 2.8828 - val_acc: 0.1803\n",
      "Epoch 4/20\n",
      "7919/7919 [==============================] - 4s - loss: 2.7972 - acc: 0.1662 - val_loss: 2.7051 - val_acc: 0.1370\n",
      "Epoch 5/20\n",
      "7919/7919 [==============================] - 4s - loss: 2.6400 - acc: 0.1673 - val_loss: 2.6035 - val_acc: 0.1405\n",
      "Epoch 6/20\n",
      "7919/7919 [==============================] - 4s - loss: 2.5404 - acc: 0.1841 - val_loss: 2.5186 - val_acc: 0.2041\n",
      "Epoch 7/20\n",
      "7919/7919 [==============================] - 4s - loss: 2.4534 - acc: 0.2339 - val_loss: 2.4411 - val_acc: 0.2218\n",
      "Epoch 8/20\n",
      "7919/7919 [==============================] - 4s - loss: 2.3486 - acc: 0.2646 - val_loss: 2.3601 - val_acc: 0.2748\n",
      "Epoch 9/20\n",
      "7919/7919 [==============================] - 4s - loss: 2.2350 - acc: 0.2966 - val_loss: 2.2397 - val_acc: 0.2895\n",
      "Epoch 10/20\n",
      "7919/7919 [==============================] - 4s - loss: 2.1338 - acc: 0.3498 - val_loss: 2.1641 - val_acc: 0.2742\n",
      "Epoch 11/20\n",
      "7919/7919 [==============================] - 4s - loss: 2.0403 - acc: 0.3615 - val_loss: 2.0907 - val_acc: 0.3166\n",
      "Epoch 12/20\n",
      "7919/7919 [==============================] - 4s - loss: 1.9585 - acc: 0.3817 - val_loss: 2.0781 - val_acc: 0.3317\n",
      "Epoch 13/20\n",
      "7919/7919 [==============================] - 4s - loss: 1.8805 - acc: 0.4146 - val_loss: 2.0489 - val_acc: 0.3579\n",
      "Epoch 14/20\n",
      "7919/7919 [==============================] - 4s - loss: 1.8074 - acc: 0.4536 - val_loss: 1.9443 - val_acc: 0.3694\n",
      "Epoch 15/20\n",
      "7919/7919 [==============================] - 4s - loss: 1.7129 - acc: 0.4746 - val_loss: 1.9108 - val_acc: 0.3938\n",
      "Epoch 16/20\n",
      "7919/7919 [==============================] - 4s - loss: 1.6431 - acc: 0.5018 - val_loss: 1.8637 - val_acc: 0.4059\n",
      "Epoch 17/20\n",
      "7919/7919 [==============================] - 4s - loss: 1.5486 - acc: 0.5186 - val_loss: 1.8070 - val_acc: 0.4218\n",
      "Epoch 18/20\n",
      "7919/7919 [==============================] - 4s - loss: 1.4855 - acc: 0.5378 - val_loss: 1.7428 - val_acc: 0.4339\n",
      "Epoch 19/20\n",
      "7919/7919 [==============================] - 4s - loss: 1.3981 - acc: 0.5632 - val_loss: 1.7002 - val_acc: 0.4571\n",
      "Epoch 20/20\n",
      "7919/7919 [==============================] - 4s - loss: 1.3213 - acc: 0.5792 - val_loss: 1.6405 - val_acc: 0.4630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8dd0adee80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(train_X, train_Y, batch_size=1000,validation_split=0.3, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532/7532 [==============================] - 16s    \n",
      "[ 7  6  0 ..., 11  6 15]\n"
     ]
    }
   ],
   "source": [
    "res_class_lstm = lstm_model.predict_classes(test_X)\n",
    "print(res_class_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.79106456804\n",
      "0.412904938927\n"
     ]
    }
   ],
   "source": [
    "loss_lstm, accuracy_lstm = lstm_model.evaluate(test_X,test_Y,verbose=2)\n",
    "print(loss_lstm)\n",
    "print(accuracy_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio-Temporal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a hybrid of CNN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypbrid_model = Sequential()\n",
    "hypbrid_model.add(Embedding(input_dim=max_words_to_keep, output_dim=token_vec_size, input_length=maxlen_text))\n",
    "hypbrid_model.add(Dropout(0.2))\n",
    "hypbrid_model.add(Conv1D(64, 5, activation='relu'))\n",
    "hypbrid_model.add(MaxPooling1D(pool_size=4))\n",
    "hypbrid_model.add(LSTM(128))\n",
    "hypbrid_model.add(Dense(units=output_dim, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypbrid_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7919 samples, validate on 3395 samples\n",
      "Epoch 1/20\n",
      "7919/7919 [==============================] - 2s - loss: 2.9918 - acc: 0.0657 - val_loss: 2.9878 - val_acc: 0.0934\n",
      "Epoch 2/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.9764 - acc: 0.1118 - val_loss: 2.9736 - val_acc: 0.1025\n",
      "Epoch 3/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.9332 - acc: 0.1320 - val_loss: 2.9112 - val_acc: 0.1187\n",
      "Epoch 4/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.8253 - acc: 0.1443 - val_loss: 2.7323 - val_acc: 0.1711\n",
      "Epoch 5/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.5236 - acc: 0.1796 - val_loss: 2.4187 - val_acc: 0.1876\n",
      "Epoch 6/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.2435 - acc: 0.2296 - val_loss: 2.1854 - val_acc: 0.2430\n",
      "Epoch 7/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.0225 - acc: 0.3055 - val_loss: 1.9887 - val_acc: 0.3308\n",
      "Epoch 8/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.7684 - acc: 0.3855 - val_loss: 1.7947 - val_acc: 0.3761\n",
      "Epoch 9/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.5464 - acc: 0.4646 - val_loss: 1.6556 - val_acc: 0.4300\n",
      "Epoch 10/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.3554 - acc: 0.5376 - val_loss: 1.5499 - val_acc: 0.4642\n",
      "Epoch 11/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.1864 - acc: 0.5936 - val_loss: 1.4473 - val_acc: 0.5019\n",
      "Epoch 12/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.0249 - acc: 0.6563 - val_loss: 1.3566 - val_acc: 0.5523\n",
      "Epoch 13/20\n",
      "7919/7919 [==============================] - 1s - loss: 0.8726 - acc: 0.7205 - val_loss: 1.2742 - val_acc: 0.5859\n",
      "Epoch 14/20\n",
      "7919/7919 [==============================] - 1s - loss: 0.7408 - acc: 0.7636 - val_loss: 1.2349 - val_acc: 0.6121\n",
      "Epoch 15/20\n",
      "7919/7919 [==============================] - 1s - loss: 0.6282 - acc: 0.8045 - val_loss: 1.2028 - val_acc: 0.6336\n",
      "Epoch 16/20\n",
      "7919/7919 [==============================] - 1s - loss: 0.5476 - acc: 0.8288 - val_loss: 1.1759 - val_acc: 0.6404\n",
      "Epoch 17/20\n",
      "7919/7919 [==============================] - 1s - loss: 0.4668 - acc: 0.8582 - val_loss: 1.1576 - val_acc: 0.6515\n",
      "Epoch 18/20\n",
      "7919/7919 [==============================] - 1s - loss: 0.4162 - acc: 0.8755 - val_loss: 1.1607 - val_acc: 0.6548\n",
      "Epoch 19/20\n",
      "7919/7919 [==============================] - 1s - loss: 0.3955 - acc: 0.8832 - val_loss: 1.1918 - val_acc: 0.6536\n",
      "Epoch 20/20\n",
      "7919/7919 [==============================] - 1s - loss: 0.3582 - acc: 0.8983 - val_loss: 1.1394 - val_acc: 0.6716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8db1224c88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypbrid_model.fit(train_X, train_Y, batch_size=1000,validation_split=0.3, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prediction on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7520/7532 [============================>.] - ETA: 0s[ 7  5  0 ..., 18  6 15]\n"
     ]
    }
   ],
   "source": [
    "res_class_hybrid = hypbrid_model.predict_classes(test_X)\n",
    "print(res_class_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.53089001023\n",
      "0.573685608136\n"
     ]
    }
   ],
   "source": [
    "loss_hybrid, accuracy_hybrid = hypbrid_model.evaluate(test_X,test_Y,verbose=2)\n",
    "print(loss_hybrid)\n",
    "print(accuracy_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio-Temporal Modeling (with multiple tags as output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define,Compile and Train the Model (use sigmoid instead softmax at the last layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7919 samples, validate on 3395 samples\n",
      "Epoch 1/20\n",
      "7919/7919 [==============================] - 2s - loss: 2.9936 - acc: 0.0547 - val_loss: 2.9907 - val_acc: 0.0530\n",
      "Epoch 2/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.9839 - acc: 0.0597 - val_loss: 2.9821 - val_acc: 0.0975\n",
      "Epoch 3/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.9570 - acc: 0.1008 - val_loss: 2.9385 - val_acc: 0.1249\n",
      "Epoch 4/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.8977 - acc: 0.1357 - val_loss: 2.8430 - val_acc: 0.1420\n",
      "Epoch 5/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.7327 - acc: 0.1565 - val_loss: 2.6327 - val_acc: 0.1287\n",
      "Epoch 6/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.5638 - acc: 0.1308 - val_loss: 2.5639 - val_acc: 0.1084\n",
      "Epoch 7/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.4745 - acc: 0.1371 - val_loss: 2.4479 - val_acc: 0.1264\n",
      "Epoch 8/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.3800 - acc: 0.1561 - val_loss: 2.3949 - val_acc: 0.1526\n",
      "Epoch 9/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.3077 - acc: 0.1876 - val_loss: 2.3258 - val_acc: 0.1632\n",
      "Epoch 10/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.1989 - acc: 0.1970 - val_loss: 2.2614 - val_acc: 0.1918\n",
      "Epoch 11/20\n",
      "7919/7919 [==============================] - 1s - loss: 2.0996 - acc: 0.2175 - val_loss: 2.1565 - val_acc: 0.2186\n",
      "Epoch 12/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.9646 - acc: 0.2688 - val_loss: 2.0666 - val_acc: 0.2757\n",
      "Epoch 13/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.8416 - acc: 0.3245 - val_loss: 1.9892 - val_acc: 0.2960\n",
      "Epoch 14/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.7239 - acc: 0.3662 - val_loss: 1.9220 - val_acc: 0.3072\n",
      "Epoch 15/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.6153 - acc: 0.4019 - val_loss: 1.8683 - val_acc: 0.3588\n",
      "Epoch 16/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.5021 - acc: 0.4504 - val_loss: 1.7408 - val_acc: 0.3835\n",
      "Epoch 17/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.3712 - acc: 0.5138 - val_loss: 1.6886 - val_acc: 0.4333\n",
      "Epoch 18/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.2849 - acc: 0.5429 - val_loss: 1.6267 - val_acc: 0.4415\n",
      "Epoch 19/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.1766 - acc: 0.5876 - val_loss: 1.5328 - val_acc: 0.4916\n",
      "Epoch 20/20\n",
      "7919/7919 [==============================] - 1s - loss: 1.0676 - acc: 0.6349 - val_loss: 1.4964 - val_acc: 0.5099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8db050fe10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_model_mul_tag = Sequential()\n",
    "hybrid_model_mul_tag.add(Embedding(input_dim=max_words_to_keep, output_dim=token_vec_size, input_length=maxlen_text))\n",
    "hybrid_model_mul_tag.add(Dropout(0.2))\n",
    "hybrid_model_mul_tag.add(Conv1D(64, 5, activation='relu'))\n",
    "hybrid_model_mul_tag.add(MaxPooling1D(pool_size=4))\n",
    "hybrid_model_mul_tag.add(LSTM(128))\n",
    "hybrid_model_mul_tag.add(Dense(units=output_dim, activation='sigmoid'))\n",
    "\n",
    "hybrid_model_mul_tag.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "hybrid_model_mul_tag.fit(train_X, train_Y, batch_size=1000,validation_split=0.3, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the tags for which the prob is greater than 0.2 and use the encoder to map it back to the original tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([12]) array([1, 3, 4, 5]) array([ 0, 15, 17, 19]) ...,\n",
      " array([], dtype=int64) array([2]) array([ 0, 15, 19])]\n"
     ]
    }
   ],
   "source": [
    "threshold_tag_prob = 0.2\n",
    "res_class_prob_hybrid_model_mul_tag = hybrid_model_mul_tag.predict(test_X) > threshold_tag_prob\n",
    "res_class_hybrid_model_mul_tag = np.array([encoder.inverse_transform(res_class_prob_hybrid_model_mul_tag[i]) for i in np.ndindex(res_class_prob_hybrid_model_mul_tag.shape[:1])])\n",
    "print(res_class_hybrid_model_mul_tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
